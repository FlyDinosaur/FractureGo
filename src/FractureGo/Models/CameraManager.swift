//
//  CameraManager.swift
//  FractureGo
//
//  Created by AI Assistant
//

import Foundation
import AVFoundation
import MediaPipeTasksVision
import UIKit

/// æ‘„åƒå¤´ç®¡ç†å™¨ - å¤„ç†æ‘„åƒå¤´é¢„è§ˆå’Œæ‰‹åŠ¿æ£€æµ‹
class CameraManager: NSObject, ObservableObject {
    private var captureSession: AVCaptureSession?
    private var videoOutput: AVCaptureVideoDataOutput?
    private var previewLayer: AVCaptureVideoPreviewLayer?
    
    private let handGestureDetector = HandGestureDetector()
    private var handLandmarker: HandLandmarker?
    
    // æ‰‹åŠ¿çŠ¶æ€
    @Published var isHandClenched: Bool = false
    @Published var handLandmarks: [NormalizedLandmark] = []
    
    // å›è°ƒ
    var onHandGestureDetected: ((Bool) -> Void)?
    
    override init() {
        super.init()
        setupHandLandmarker()
    }
    
    /// è®¾ç½®æ‰‹éƒ¨å…³é”®ç‚¹æ£€æµ‹å™¨
    private func setupHandLandmarker() {
        guard let modelPath = Bundle.main.path(forResource: "hand_landmarker", ofType: "task") else {
            print("âŒ æ‰¾ä¸åˆ°æ‰‹éƒ¨å…³é”®ç‚¹æ£€æµ‹æ¨¡å‹æ–‡ä»¶")
            return
        }
        
        let options = HandLandmarkerOptions()
        options.baseOptions.modelAssetPath = modelPath
        options.runningMode = .liveStream
        options.numHands = 1
        options.minHandDetectionConfidence = 0.5
        options.minHandPresenceConfidence = 0.5
        options.minTrackingConfidence = 0.5
        
        // è®¾ç½®ç»“æœå›è°ƒ
        options.handLandmarkerLiveStreamDelegate = self
        
        do {
            handLandmarker = try HandLandmarker(options: options)
            print("âœ… æ‰‹éƒ¨å…³é”®ç‚¹æ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
        } catch {
            print("âŒ æ‰‹éƒ¨å…³é”®ç‚¹æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: \(error)")
        }
    }
    
    /// å¯åŠ¨æ‘„åƒå¤´ä¼šè¯
    func startSession() {
        guard captureSession == nil else {
            print("âš ï¸ æ‘„åƒå¤´ä¼šè¯å·²ç»å­˜åœ¨ï¼Œè·³è¿‡å¯åŠ¨")
            return
        }
        
        print("ğŸ¥ å¼€å§‹å¯åŠ¨æ‘„åƒå¤´ä¼šè¯...")
        
        DispatchQueue.global(qos: .userInitiated).async { [weak self] in
            self?.setupCaptureSession()
        }
    }
    
    /// åœæ­¢æ‘„åƒå¤´ä¼šè¯
    func stopSession() {
        captureSession?.stopRunning()
        captureSession = nil
        previewLayer = nil
    }
    
    /// è®¾ç½®æ‘„åƒå¤´æ•è·ä¼šè¯
    private func setupCaptureSession() {
        print("ğŸ”§ å¼€å§‹è®¾ç½®æ‘„åƒå¤´æ•è·ä¼šè¯...")
        
        let session = AVCaptureSession()
        session.sessionPreset = .medium
        print("ğŸ“± è®¾ç½®ä¼šè¯é¢„è®¾ä¸º medium")
        
        // æ·»åŠ å‰ç½®æ‘„åƒå¤´è¾“å…¥
        guard let frontCamera = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .front),
              let input = try? AVCaptureDeviceInput(device: frontCamera) else {
            print("âŒ æ— æ³•è®¿é—®å‰ç½®æ‘„åƒå¤´")
            return
        }
        
        print("âœ… æˆåŠŸè·å–å‰ç½®æ‘„åƒå¤´è®¾å¤‡: \(frontCamera.localizedName)")
        
        if session.canAddInput(input) {
            session.addInput(input)
            print("âœ… æˆåŠŸæ·»åŠ æ‘„åƒå¤´è¾“å…¥")
        } else {
            print("âŒ æ— æ³•æ·»åŠ æ‘„åƒå¤´è¾“å…¥åˆ°ä¼šè¯")
            return
        }
        
        // æ·»åŠ è§†é¢‘è¾“å‡º
        let output = AVCaptureVideoDataOutput()
        output.setSampleBufferDelegate(self, queue: DispatchQueue.global(qos: .userInitiated))
        output.videoSettings = [kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_32BGRA]
        print("ğŸ¬ é…ç½®è§†é¢‘è¾“å‡ºè®¾ç½®")
        
        if session.canAddOutput(output) {
            session.addOutput(output)
            videoOutput = output
            print("âœ… æˆåŠŸæ·»åŠ è§†é¢‘è¾“å‡º")
        } else {
            print("âŒ æ— æ³•æ·»åŠ è§†é¢‘è¾“å‡ºåˆ°ä¼šè¯")
            return
        }
        
        // åˆ›å»ºé¢„è§ˆå±‚
        let preview = AVCaptureVideoPreviewLayer(session: session)
        preview.videoGravity = .resizeAspectFill
        print("ğŸ–¼ï¸ åˆ›å»ºé¢„è§ˆå±‚")
        
        DispatchQueue.main.async {
            self.previewLayer = preview
            self.captureSession = session
            session.startRunning()
            print("âœ… æ‘„åƒå¤´ä¼šè¯å¯åŠ¨æˆåŠŸï¼Œå¼€å§‹è¿è¡Œ")
            print("ğŸ“Š ä¼šè¯çŠ¶æ€: \(session.isRunning ? "è¿è¡Œä¸­" : "å·²åœæ­¢")")
        }
    }
    
    /// è·å–é¢„è§ˆå±‚
    func getPreviewLayer() -> AVCaptureVideoPreviewLayer? {
        return previewLayer
    }
}

// MARK: - AVCaptureVideoDataOutputSampleBufferDelegate
extension CameraManager: AVCaptureVideoDataOutputSampleBufferDelegate {
    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        guard let handLandmarker = handLandmarker else { return }
        
        // è½¬æ¢ä¸ºMPImage
        guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { return }
        
        do {
            let mpImage = try MPImage(pixelBuffer: pixelBuffer)
            
            // è·å–æ—¶é—´æˆ³
            let timestamp = CMSampleBufferGetPresentationTimeStamp(sampleBuffer)
            let timestampMs = Int(CMTimeGetSeconds(timestamp) * 1000)
            
            // å¼‚æ­¥æ£€æµ‹æ‰‹éƒ¨å…³é”®ç‚¹
            try handLandmarker.detectAsync(image: mpImage, timestampInMilliseconds: timestampMs)
        } catch {
             print("âŒ æ‰‹éƒ¨å…³é”®ç‚¹æ£€æµ‹å¤±è´¥: \(error)")
         }
    }
}

// MARK: - HandLandmarkerLiveStreamDelegate
extension CameraManager: HandLandmarkerLiveStreamDelegate {
    func handLandmarker(_ handLandmarker: HandLandmarker, didFinishDetection result: HandLandmarkerResult?, timestampInMilliseconds: Int, error: Error?) {
        if let error = error {
            print("âŒ æ‰‹éƒ¨æ£€æµ‹é”™è¯¯: \(error)")
            return
        }
        
        guard let result = result else {
            print("âš ï¸ æ‰‹éƒ¨æ£€æµ‹ç»“æœä¸ºç©º")
            DispatchQueue.main.async {
                self.isHandClenched = false
                self.handLandmarks = []
                self.onHandGestureDetected?(false)
            }
            return
        }
        
        if result.landmarks.isEmpty {
            print("âš ï¸ æœªæ£€æµ‹åˆ°æ‰‹éƒ¨å…³é”®ç‚¹")
            DispatchQueue.main.async {
                self.isHandClenched = false
                self.handLandmarks = []
                self.onHandGestureDetected?(false)
            }
            return
        }
        
        let firstHand = result.landmarks[0]
        print("âœ… æ£€æµ‹åˆ°æ‰‹éƒ¨å…³é”®ç‚¹ï¼Œæ•°é‡: \(firstHand.count)")
        
        // æ£€æµ‹æ¡æ‹³çŠ¶æ€
        let isClenched = handGestureDetector.isHandClenched(landmarks: firstHand)
        print("ğŸ¤œ æ¡æ‹³æ£€æµ‹ç»“æœ: \(isClenched ? "æ¡æ‹³" : "å¼ å¼€")")
        
        // æ‰“å°å…³é”®ç‚¹ä½ç½®ç”¨äºè°ƒè¯•
        if firstHand.count >= 21 {
            let wrist = firstHand[0]
            let thumbTip = firstHand[4]
            let indexTip = firstHand[8]
            let middleTip = firstHand[12]
            print("ğŸ“ å…³é”®ç‚¹ä½ç½® - æ‰‹è…•: (\(String(format: "%.3f", wrist.x)), \(String(format: "%.3f", wrist.y))), æ‹‡æŒ‡å°–: (\(String(format: "%.3f", thumbTip.x)), \(String(format: "%.3f", thumbTip.y))), é£ŸæŒ‡å°–: (\(String(format: "%.3f", indexTip.x)), \(String(format: "%.3f", indexTip.y))), ä¸­æŒ‡å°–: (\(String(format: "%.3f", middleTip.x)), \(String(format: "%.3f", middleTip.y)))")
        }
        
        DispatchQueue.main.async {
            self.isHandClenched = isClenched
            self.handLandmarks = firstHand
            self.onHandGestureDetected?(isClenched)
        }
    }
}